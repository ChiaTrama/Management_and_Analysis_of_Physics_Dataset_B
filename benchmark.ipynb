{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d5422f-df61-4c8a-8f9a-2c8aab9f79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.cluster import KMeans\n",
    "import numpy as np\n",
    "from dask_ml.metrics import pairwise_distances\n",
    "from dask_ml.metrics import pairwise_distances_argmin_min\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "from dask_ml.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from dask.distributed import SSHCluster\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a568cb72-44bb-4010-a345-5a90473ca9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(X,C):\n",
    "    return pairwise_distances(X, C, metric='sqeuclidean').min(1).sum() \n",
    "    # Compute the sum of distances to the nearest centroid at axis=1\n",
    "    \n",
    "def kmeans_parallel_init_dask(X, k, l):\n",
    "    # Step 1: Randomly select a point from X\n",
    "    n, d = X.shape\n",
    "    idx = np.random.choice(n, size=1)\n",
    "    C = X[idx].compute()  # Collect to memory for use\n",
    "\n",
    "    # Step 2: Compute φX(C)\n",
    "    phi_X_C = phi(X, C).compute() # Compute the sum of distances to the nearest centroid\n",
    "\n",
    "    # Steps 3-6: Repeat O(log φX(C)) times\n",
    "    rounds = int(np.log(phi_X_C))\n",
    "    #print(f\"Begin centroid sampling with number of rounds: {rounds}\")\n",
    "    for _ in range(rounds):\n",
    "        dist_sq = pairwise_distances(X, C, metric='sqeuclidean').min(1)\n",
    "        dist_sum = dist_sq.sum()\n",
    "        p_x = l * dist_sq / dist_sum\n",
    "        samples = da.random.uniform(size=len(p_x), chunks=p_x.chunks) < p_x\n",
    "        sampled_idx = da.where(samples)[0].compute()\n",
    "        \n",
    "        new_C = X[sorted(sampled_idx)].compute() #https://github.com/dask/dask-ml/issues/39 \n",
    "        C = np.vstack((C, new_C))\n",
    "\n",
    "    # Step 7: Compute weights\n",
    "    dist_to_C = pairwise_distances(X, C, metric='euclidean')\n",
    "    closest_C = da.argmin(dist_to_C, axis=1)\n",
    "\n",
    "    weights = np.empty(len(C))\n",
    "    counts = da.bincount(closest_C, minlength=len(C)).compute()\n",
    "    weights[:len(counts)] = counts\n",
    "    \n",
    "    # Normalize weights so that they sum up to the number of centroids\n",
    "    weight_sum = np.sum(weights)\n",
    "    if weight_sum == 0:\n",
    "        raise ValueError(\"Sum of weights is zero, cannot normalize.\")\n",
    "    \n",
    "    weights_normalized = weights / weight_sum\n",
    "    \n",
    "    dask_C = da.from_array(C, chunks=(C.shape[0], C.shape[1])) # here we ensure that the re-clustering occurs on a single-thread\n",
    "\n",
    "    # Step 8: Recluster the weighted points in C into k clusters\n",
    "    #print(\"Begin centroid re-clustering\")\n",
    "    labels, centroids = lloyd_kmeans_plusplus(X=dask_C, weights=weights_normalized,k=k, max_iters=10, tol=1e-8)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f705038a-77ba-496e-86a8-dd9c950b0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids_weighted(X, labels, weights, k):\n",
    "    \"\"\"\n",
    "    Update the centroids by computing the weighted mean of the points assigned to each cluster.\n",
    "    \"\"\"\n",
    "    centroids = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Select the points that are assigned to cluster i\n",
    "        cluster_points = X[labels == i]\n",
    "        \n",
    "        # Select the corresponding weights for these points\n",
    "        cluster_weights = weights[labels == i]\n",
    "        \n",
    "        if len(cluster_points) == 0:\n",
    "            # If no points are assigned to this cluster, avoid division by zero\n",
    "            # Continue without updating that centroid\n",
    "            continue\n",
    "\n",
    "        # Compute the weighted mean using dask.array.average\n",
    "        weighted_mean = da.average(cluster_points, axis=0, weights=cluster_weights)\n",
    "        \n",
    "        # Append the computed weighted mean to the centroids list\n",
    "        centroids.append(weighted_mean)\n",
    "    \n",
    "    # Convert centroids list to Dask array\n",
    "    return da.stack(centroids)\n",
    "\n",
    "def kmeans_plusplus_init(X, weights, k):\n",
    "    '''\n",
    "    K-means++ initialization to select k initial centroids from X as a numpy array, keeping C as a NumPy array and weighting by provided weights.\n",
    "    '''\n",
    "    n, d = X.shape\n",
    "    # Step 1: Randomly select the first centroid\n",
    "    idx = np.random.choice(n, size=1)\n",
    "    C = X[idx].compute()\n",
    "\n",
    "    for _ in range(1, k):\n",
    "        # Step 2: Compute distances from each point to the nearest centroid\n",
    "        # C is a NumPy array, X is a Dask array\n",
    "        # Compute the distances from each point to the nearest centroid normalizing by weights\n",
    "        distances = pairwise_distances(X, C, metric='sqeuclidean').min(1) * (weights)\n",
    "        \n",
    "        # Compute the probabilities for choosing each point\n",
    "        probabilities = distances / distances.sum()\n",
    "        \n",
    "        # Sample a new point based on these probabilities\n",
    "        new_idx = np.random.choice(n, size=1, p=probabilities)\n",
    "        new_centroid = X[sorted(new_idx)].compute()\n",
    "        \n",
    "        # Add the new centroid to the list\n",
    "        C = np.vstack((C, new_centroid))\n",
    "    return C\n",
    "\n",
    "def lloyd_kmeans_plusplus(X, weights, k, max_iters=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Lloyd's algorithm for k-means clustering using Dask weighting the mean to update centroids.\n",
    "    \"\"\"\n",
    "    centroids = kmeans_plusplus_init(X, weights, k)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        labels = assign_clusters(X, centroids).compute()\n",
    "        new_centroids = update_centroids_weighted(X, labels, weights=weights, k=k).compute()\n",
    "        \n",
    "        if da.allclose(centroids, new_centroids, atol=tol).compute():\n",
    "            #print(f\"Centroid Lloyd Converged after {i+1} iterations.\")\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f296baa-c2f9-475a-ad03-7d1a98e4efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(X, centroids):\n",
    "    \"\"\"\n",
    "    Assign each point to the nearest centroid using Dask to parallelize the computation.\n",
    "    \"\"\"\n",
    "    return pairwise_distances_argmin_min(X, centroids, metric='sqeuclidean')[0]\n",
    "\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update the centroids by computing the mean of the points assigned to each cluster with Dask.\n",
    "    \"\"\"\n",
    "    centroids = da.stack([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "    return centroids\n",
    "    \n",
    "def kmeans_parallel(X, k, max_iters=100, tol=1e-8, l=2):\n",
    "    centroids = kmeans_parallel_init_dask(X, k, l)\n",
    "    for i in range(max_iters):\n",
    "        labels = assign_clusters(X, centroids)\n",
    "        new_centroids = update_centroids(X, labels, k).compute()\n",
    "\n",
    "        if da.allclose(centroids, new_centroids, atol=tol):\n",
    "            #print(f\"Main KMeans Converged after {i+1} iterations.\")\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b78017-a670-48b7-a198-bb2f22022141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the KDDCUP99 dataset in a pd dataframe\n",
    "file_path = '/opt/kddcup99/kddcup.data.gz'\n",
    "df = pd.read_csv(file_path, compression='gzip', header=None)\n",
    "\n",
    "# We identified those as the non-numerical columns\n",
    "columns_to_drop = [1, 2, 3, 41]\n",
    "\n",
    "# We remove those columns to accomodate all or part of the dataset into an array\n",
    "df = df.drop(df.columns[columns_to_drop], axis=1)\n",
    "num_rows = len(df)\n",
    "\n",
    "num_sampled = int(num_rows * 1) #we can change the percentage of the dataset to sample\n",
    "\n",
    "# And take only up to the desired part\n",
    "df= df.iloc[:num_sampled]\n",
    "\n",
    "# We initialize a numpy array ready to convert to a dask array during benchmark\n",
    "data = df.values\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d39c1350-d246-4b8e-93cc-2a162f43eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmarking function with multiple runs\n",
    "def benchmark_kmeans(data, n_workers, threads_per_worker, memory_limit, chunks, n_runs):\n",
    "\n",
    "\n",
    "    # List of IP addresses for the virtual machines (VMs) where Dask workers will run\n",
    "    VM_IPS = ['10.67.22.89','10.67.22.89', '10.67.22.241', '10.67.22.171']\n",
    "    #VM_IPS = ['10.67.22.89', '10.67.22.241', '10.67.22.171']\n",
    "\n",
    "    # Initialize an SSHCluster with the specified VMs\n",
    "    cluster = SSHCluster(\n",
    "        hosts=VM_IPS,  # IP addresses of the VMs\n",
    "        connect_options={\n",
    "            \"username\": \"tramarin\"  # Username used to SSH into the VMs\n",
    "        },\n",
    "        worker_options={\n",
    "            \"n_workers\": n_workers,  # Number of worker processes to launch per VM\n",
    "            \"nthreads\":  threads_per_worker ,   # Number of threads per worker process\n",
    "            \"memory_limit\": memory_limit\n",
    "        },\n",
    "        scheduler_options={\n",
    "            \"port\": 8786,  # Port on the scheduler VM for communication with workers\n",
    "            \"dashboard_address\": 8787,  # Port on the scheduler VM for the Dask dashboard\n",
    "        },\n",
    "    )\n",
    "\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(f\"\\nBenchmarking with {n_workers} workers, {threads_per_worker} threads per worker, \"\n",
    "          f\"memory limit {memory_limit}, chunks {chunks}\")\n",
    "\n",
    "    # Step 2: import ddk as dask array\n",
    "\n",
    "    dask_array = da.from_array(data, chunks=(data.shape[0] // chunks, data.shape[1]))\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True) # for scaling with a sparse matrix\n",
    "    # we normalize the data to fasten clustering\n",
    "    data_normalized = scaler.fit_transform(dask_array)\n",
    "    del dask_array\n",
    "    # persist the data to avoid re-computation in the benchmark\n",
    "    data_normalized = data_normalized.persist()\n",
    "\n",
    "    centers = 4\n",
    "\n",
    "    # Step 3: Benchmark custom KMeans function\n",
    "    custom_kmeans_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start_time = timer()\n",
    "        synt_labels, synt_centroids = kmeans_parallel(X=data_normalized, k=centers, l=2)\n",
    "        custom_kmeans_times.append(timer() - start_time)\n",
    "    \n",
    "    custom_kmeans_mean_time = np.mean(custom_kmeans_times)\n",
    "    custom_kmeans_std_time = np.std(custom_kmeans_times)\n",
    "\n",
    "    # Step 4: Benchmark Dask-ML KMeans\n",
    "    dask_ml_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start_time = timer()\n",
    "        dask_ml_kmeans = KMeans(n_clusters=centers).fit(data_normalized)\n",
    "        dask_ml_times.append(timer() - start_time)\n",
    "\n",
    "    dask_ml_mean_time = np.mean(dask_ml_times)\n",
    "    dask_ml_std_time = np.std(dask_ml_times)\n",
    "\n",
    "\n",
    "    # Record results\n",
    "    results = {\n",
    "        'workers': n_workers,\n",
    "        'threads': threads_per_worker,\n",
    "        'memory_limit': memory_limit,\n",
    "        'chunks': chunks,\n",
    "        'custom_kmeans_mean_time': custom_kmeans_mean_time,\n",
    "        'custom_kmeans_std_time': custom_kmeans_std_time,\n",
    "        'dask_ml_mean_time': dask_ml_mean_time,\n",
    "        'dask_ml_std_time': dask_ml_std_time,\n",
    "       \n",
    "    }\n",
    "\n",
    "    print(f\"Custom KMeans Mean Time: {custom_kmeans_mean_time:.2f} seconds, Std: {custom_kmeans_std_time:.2f} seconds\")\n",
    "    print(f\"Dask-ML KMeans Mean Time: {dask_ml_mean_time:.2f} seconds, Std: {dask_ml_std_time:.2f} seconds\")\n",
    "    \n",
    "    \n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab83067e-40f0-4eb8-98b9-b8441a861180",
   "metadata": {},
   "source": [
    "# Define the configurations to test\n",
    "worker_configs = [\n",
    "    {'n_workers': 1, 'threads_per_worker': 4, 'memory_limit': '7GB'},#attenta cambia\n",
    "]\n",
    "\n",
    "chunk_sizes = [\n",
    "    48\n",
    "    #12,24,48,192 #whole dataset is 264MiB, so 8 chunks is 37MiB, 16 chunks is 18MiB, qui pensaci\n",
    "]\n",
    "\n",
    "# Run benchmark for each configuration and chunk size\n",
    "all_results = []\n",
    "for config in worker_configs:\n",
    "    for chunks in chunk_sizes:\n",
    "        result = benchmark_kmeans(\n",
    "            data=data,\n",
    "            n_workers=config['n_workers'],\n",
    "            threads_per_worker=config['threads_per_worker'],\n",
    "            memory_limit=config['memory_limit'],\n",
    "            chunks=chunks,\n",
    "            n_runs=8 #only pari numeri\n",
    "        )\n",
    "        all_results.append(result)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5153c133-04af-49ff-aef4-727f79734ac7",
   "metadata": {},
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save results to a file in Parquet format\n",
    "results_df.to_pickle('benchmark_results_cluster_all_dataset_1w4t7gb_chunks_48.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b50b10-6a2d-43cc-968a-578a922d876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 17:47:35,724 - distributed.deploy.ssh - INFO - 2024-09-01 17:47:35,724 - distributed.scheduler - INFO - State start\n",
      "2024-09-01 17:47:35,729 - distributed.deploy.ssh - INFO - 2024-09-01 17:47:35,728 - distributed.scheduler - INFO -   Scheduler at:    tcp://10.67.22.89:8786\n",
      "2024-09-01 17:47:36,828 - distributed.deploy.ssh - INFO - 2024-09-01 17:47:36,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.89:41109'\n",
      "2024-09-01 17:47:37,105 - distributed.deploy.ssh - INFO - 2024-09-01 17:47:37,116 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.171:35669'\n",
      "2024-09-01 17:47:37,141 - distributed.deploy.ssh - INFO - 2024-09-01 17:47:37,141 - distributed.worker - INFO -       Start worker at:    tcp://10.67.22.89:37113\n",
      "2024-09-01 17:47:37,155 - distributed.deploy.ssh - INFO - 2024-09-01 17:47:37,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.241:46589'\n",
      "2024-09-01 17:47:37,411 - distributed.deploy.ssh - INFO - 2024-09-01 17:47:37,421 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.171:37011\n",
      "2024-09-01 17:47:37,477 - distributed.deploy.ssh - INFO - 2024-09-01 17:47:37,484 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.241:42901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking with 1 workers, 4 threads per worker, memory limit 7 GB, chunks 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom KMeans Mean Time: 8.61 seconds, Std: 2.09 seconds\n",
      "Dask-ML KMeans Mean Time: 6.92 seconds, Std: 0.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 17:51:27,239 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:27,238 - distributed.scheduler - INFO - State start\n",
      "2024-09-01 17:51:27,240 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:27,239 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pdhndhtw', purging\n",
      "2024-09-01 17:51:27,242 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:27,242 - distributed.scheduler - INFO -   Scheduler at:    tcp://10.67.22.89:8786\n",
      "2024-09-01 17:51:28,349 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.89:34777'\n",
      "2024-09-01 17:51:28,352 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.89:44721'\n",
      "2024-09-01 17:51:28,354 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.89:41297'\n",
      "2024-09-01 17:51:28,565 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,565 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.171:42321'\n",
      "2024-09-01 17:51:28,568 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.171:39369'\n",
      "2024-09-01 17:51:28,569 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.171:38343'\n",
      "2024-09-01 17:51:28,637 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.241:45161'\n",
      "2024-09-01 17:51:28,641 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.241:38261'\n",
      "2024-09-01 17:51:28,644 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.241:41303'\n",
      "2024-09-01 17:51:28,666 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,666 - distributed.worker - INFO -       Start worker at:    tcp://10.67.22.89:46319\n",
      "2024-09-01 17:51:28,667 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,666 - distributed.worker - INFO -          Listening to:    tcp://10.67.22.89:46319\n",
      "2024-09-01 17:51:28,668 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,666 - distributed.worker - INFO -          dashboard at:          10.67.22.89:38465\n",
      "2024-09-01 17:51:28,669 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,666 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:51:28,669 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,666 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,670 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,667 - distributed.worker - INFO -               Threads:                          1\n",
      "2024-09-01 17:51:28,671 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,667 - distributed.worker - INFO -                Memory:                   2.42 GiB\n",
      "2024-09-01 17:51:28,672 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,667 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z3d7jaix\n",
      "2024-09-01 17:51:28,673 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,667 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,683 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,682 - distributed.worker - INFO -       Start worker at:    tcp://10.67.22.89:41435\n",
      "2024-09-01 17:51:28,684 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,683 - distributed.worker - INFO -          Listening to:    tcp://10.67.22.89:41435\n",
      "2024-09-01 17:51:28,684 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,683 - distributed.worker - INFO -          dashboard at:          10.67.22.89:33917\n",
      "2024-09-01 17:51:28,684 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,683 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:51:28,685 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,683 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,685 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,683 - distributed.worker - INFO -               Threads:                          1\n",
      "2024-09-01 17:51:28,686 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,683 - distributed.worker - INFO -                Memory:                   2.42 GiB\n",
      "2024-09-01 17:51:28,686 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,683 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wzat_as1\n",
      "2024-09-01 17:51:28,687 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,683 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,688 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,688 - distributed.worker - INFO -       Start worker at:    tcp://10.67.22.89:34845\n",
      "2024-09-01 17:51:28,860 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,859 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-ld8vkgey', purging\n",
      "2024-09-01 17:51:28,871 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,870 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.171:36585\n",
      "2024-09-01 17:51:28,871 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,870 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.171:36585\n",
      "2024-09-01 17:51:28,872 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,870 - distributed.worker - INFO -          dashboard at:         10.67.22.171:46857\n",
      "2024-09-01 17:51:28,873 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,870 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:51:28,874 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,870 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,875 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,870 - distributed.worker - INFO -               Threads:                          1\n",
      "2024-09-01 17:51:28,875 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,870 - distributed.worker - INFO -                Memory:                   2.42 GiB\n",
      "2024-09-01 17:51:28,876 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,870 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-1000/worker-ivso9xas\n",
      "2024-09-01 17:51:28,877 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,871 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,883 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.171:38557\n",
      "2024-09-01 17:51:28,884 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.171:38557\n",
      "2024-09-01 17:51:28,884 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO -          dashboard at:         10.67.22.171:37949\n",
      "2024-09-01 17:51:28,885 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:51:28,886 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,886 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO -               Threads:                          1\n",
      "2024-09-01 17:51:28,887 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO -                Memory:                   2.42 GiB\n",
      "2024-09-01 17:51:28,888 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-1000/worker-2cobarns\n",
      "2024-09-01 17:51:28,888 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,883 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,889 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,888 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.171:43557\n",
      "2024-09-01 17:51:28,954 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,951 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-ilyypnbq', purging\n",
      "2024-09-01 17:51:28,966 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.241:43147\n",
      "2024-09-01 17:51:28,966 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.241:43147\n",
      "2024-09-01 17:51:28,967 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO -          dashboard at:         10.67.22.241:46519\n",
      "2024-09-01 17:51:28,968 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:51:28,968 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,969 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO -               Threads:                          1\n",
      "2024-09-01 17:51:28,970 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO -                Memory:                   2.42 GiB\n",
      "2024-09-01 17:51:28,971 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-1000/worker-mmh5ez47\n",
      "2024-09-01 17:51:28,972 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,963 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,983 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.241:45305\n",
      "2024-09-01 17:51:28,984 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.241:45305\n",
      "2024-09-01 17:51:28,984 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO -          dashboard at:         10.67.22.241:43039\n",
      "2024-09-01 17:51:28,984 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:51:28,985 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,987 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO -               Threads:                          1\n",
      "2024-09-01 17:51:28,987 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO -                Memory:                   2.42 GiB\n",
      "2024-09-01 17:51:28,988 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-1000/worker-3q2eaghi\n",
      "2024-09-01 17:51:28,988 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,980 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:51:28,992 - distributed.deploy.ssh - INFO - 2024-09-01 17:51:28,989 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.241:40629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking with 3 workers, 1 threads per worker, memory limit 2.6GB, chunks 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom KMeans Mean Time: 10.72 seconds, Std: 2.13 seconds\n",
      "Dask-ML KMeans Mean Time: 8.19 seconds, Std: 0.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 17:56:06,228 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:06,227 - distributed.scheduler - INFO - State start\n",
      "2024-09-01 17:56:06,229 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:06,228 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fhohgaib', purging\n",
      "2024-09-01 17:56:06,230 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:06,228 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-z3d7jaix', purging\n",
      "2024-09-01 17:56:06,231 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:06,228 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-wzat_as1', purging\n",
      "2024-09-01 17:56:06,232 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:06,231 - distributed.scheduler - INFO -   Scheduler at:    tcp://10.67.22.89:8786\n",
      "2024-09-01 17:56:07,328 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,328 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.89:46131'\n",
      "2024-09-01 17:56:07,330 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,330 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.89:41551'\n",
      "2024-09-01 17:56:07,563 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.171:45817'\n",
      "2024-09-01 17:56:07,565 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.171:39515'\n",
      "2024-09-01 17:56:07,592 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,592 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.241:43131'\n",
      "2024-09-01 17:56:07,595 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.241:42893'\n",
      "2024-09-01 17:56:07,649 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,648 - distributed.worker - INFO -       Start worker at:    tcp://10.67.22.89:45851\n",
      "2024-09-01 17:56:07,650 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO -          Listening to:    tcp://10.67.22.89:45851\n",
      "2024-09-01 17:56:07,650 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO -          dashboard at:          10.67.22.89:41929\n",
      "2024-09-01 17:56:07,651 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:56:07,651 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:56:07,652 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO -               Threads:                          2\n",
      "2024-09-01 17:56:07,652 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO -                Memory:                   2.98 GiB\n",
      "2024-09-01 17:56:07,653 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m9mv8yq1\n",
      "2024-09-01 17:56:07,653 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:56:07,653 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,649 - distributed.worker - INFO -       Start worker at:    tcp://10.67.22.89:44691\n",
      "2024-09-01 17:56:07,861 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,861 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-ivso9xas', purging\n",
      "2024-09-01 17:56:07,861 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,862 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-2cobarns', purging\n",
      "2024-09-01 17:56:07,862 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,862 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-flx2vwvc', purging\n",
      "2024-09-01 17:56:07,872 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.171:45775\n",
      "2024-09-01 17:56:07,873 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.171:45775\n",
      "2024-09-01 17:56:07,873 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO -          dashboard at:         10.67.22.171:35649\n",
      "2024-09-01 17:56:07,873 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:56:07,874 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:56:07,874 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO -               Threads:                          2\n",
      "2024-09-01 17:56:07,874 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO -                Memory:                   2.98 GiB\n",
      "2024-09-01 17:56:07,875 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-1000/worker-kd769plj\n",
      "2024-09-01 17:56:07,875 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,873 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:56:07,877 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,878 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.171:40395\n",
      "2024-09-01 17:56:07,902 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,902 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-3q2eaghi', purging\n",
      "2024-09-01 17:56:07,903 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,902 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-f3jsqr_c', purging\n",
      "2024-09-01 17:56:07,903 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,902 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-mmh5ez47', purging\n",
      "2024-09-01 17:56:07,914 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.241:42709\n",
      "2024-09-01 17:56:07,915 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.241:42709\n",
      "2024-09-01 17:56:07,915 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO -          dashboard at:         10.67.22.241:43425\n",
      "2024-09-01 17:56:07,915 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO - Waiting to connect to:     tcp://10.67.22.89:8786\n",
      "2024-09-01 17:56:07,916 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:56:07,916 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO -               Threads:                          2\n",
      "2024-09-01 17:56:07,917 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO -                Memory:                   2.98 GiB\n",
      "2024-09-01 17:56:07,917 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-1000/worker-u3hhvsj0\n",
      "2024-09-01 17:56:07,917 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,914 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-09-01 17:56:07,944 - distributed.deploy.ssh - INFO - 2024-09-01 17:56:07,944 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.241:43773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking with 2 workers, 2 threads per worker, memory limit 3.2 GB, chunks 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom KMeans Mean Time: 9.95 seconds, Std: 1.91 seconds\n",
      "Dask-ML KMeans Mean Time: 6.13 seconds, Std: 0.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:00:04,631 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:04,630 - distributed.scheduler - INFO - State start\n",
      "2024-09-01 18:00:04,632 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:04,631 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-m9mv8yq1', purging\n",
      "2024-09-01 18:00:04,632 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:04,631 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cn78v34t', purging\n",
      "2024-09-01 18:00:04,634 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:04,634 - distributed.scheduler - INFO -   Scheduler at:    tcp://10.67.22.89:8786\n",
      "2024-09-01 18:00:05,734 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:05,733 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.89:36493'\n",
      "2024-09-01 18:00:05,968 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:05,969 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.171:35713'\n",
      "2024-09-01 18:00:06,026 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:06,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.241:36611'\n",
      "2024-09-01 18:00:06,047 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:06,046 - distributed.worker - INFO -       Start worker at:    tcp://10.67.22.89:34001\n",
      "2024-09-01 18:00:06,266 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:06,267 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-uofely7b', purging\n",
      "2024-09-01 18:00:06,267 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:06,267 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-kd769plj', purging\n",
      "2024-09-01 18:00:06,278 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:06,279 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.171:32969\n",
      "2024-09-01 18:00:06,332 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:06,332 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-u3hhvsj0', purging\n",
      "2024-09-01 18:00:06,333 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:06,332 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-kq1w3p83', purging\n",
      "2024-09-01 18:00:06,343 - distributed.deploy.ssh - INFO - 2024-09-01 18:00:06,344 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.241:37159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking with 1 workers, 3 threads per worker, memory limit 7 GB, chunks 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom KMeans Mean Time: 10.97 seconds, Std: 3.44 seconds\n",
      "Dask-ML KMeans Mean Time: 6.93 seconds, Std: 0.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:04:29,426 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:29,426 - distributed.scheduler - INFO - State start\n",
      "2024-09-01 18:04:29,427 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:29,426 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nx7v7xl9', purging\n",
      "2024-09-01 18:04:29,430 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:29,429 - distributed.scheduler - INFO -   Scheduler at:    tcp://10.67.22.89:8786\n",
      "2024-09-01 18:04:30,538 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:30,538 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.89:35681'\n",
      "2024-09-01 18:04:30,752 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:30,753 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.171:44291'\n",
      "2024-09-01 18:04:30,777 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:30,778 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.241:34197'\n",
      "2024-09-01 18:04:30,854 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:30,854 - distributed.worker - INFO -       Start worker at:    tcp://10.67.22.89:40291\n",
      "2024-09-01 18:04:31,049 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:31,050 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-6ebkhhax', purging\n",
      "2024-09-01 18:04:31,060 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:31,061 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.171:42007\n",
      "2024-09-01 18:04:31,082 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:31,082 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-1000/worker-t8p9ttjx', purging\n",
      "2024-09-01 18:04:31,094 - distributed.deploy.ssh - INFO - 2024-09-01 18:04:31,094 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.241:38909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking with 1 workers, 2 threads per worker, memory limit 7 GB, chunks 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 1.11 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "/opt/daskenv1/lib/python3.12/site-packages/dask/base.py:1539: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom KMeans Mean Time: 11.11 seconds, Std: 3.37 seconds\n",
      "Dask-ML KMeans Mean Time: 7.54 seconds, Std: 0.42 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the configurations to test\n",
    "worker_configs = [\n",
    "    {'n_workers': 1, 'threads_per_worker': 4, 'memory_limit': '7 GB'}, \n",
    "    {'n_workers': 3, 'threads_per_worker': 1, 'memory_limit': '2.6GB'},\n",
    "    {'n_workers': 2, 'threads_per_worker': 2 , 'memory_limit': '3.2 GB'},\n",
    "    {'n_workers': 1, 'threads_per_worker': 3 , 'memory_limit': '7 GB'},\n",
    "    {'n_workers': 1, 'threads_per_worker': 2 , 'memory_limit': '7 GB'},\n",
    "]\n",
    "\n",
    "chunk_sizes = [\n",
    "    12 \n",
    "]\n",
    "\n",
    "# Run benchmark for each configuration and chunk size\n",
    "all_results = []\n",
    "for config in worker_configs:\n",
    "    for chunks in chunk_sizes:\n",
    "        result = benchmark_kmeans(\n",
    "            data=data,\n",
    "            n_workers=config['n_workers'],\n",
    "            threads_per_worker=config['threads_per_worker'],\n",
    "            memory_limit=config['memory_limit'],\n",
    "            chunks=chunks,\n",
    "            n_runs=14 \n",
    "        )\n",
    "        all_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d95edf4-d514-4ea1-bff7-5a3cbb9bcd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save results to a file in Parquet format\n",
    "results_df.to_pickle('benchmark_results_cluster_config_80PERCDATA_14runs.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
